{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2eef629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: CIFAR10\n",
    "\n",
    "# Architecture: simplified Vision Transformer (ViT) to classify images\n",
    "# Loss: CrossEntropyLoss\n",
    "# Optimizer: Adam (lr=0.001)\n",
    "\n",
    "# Define the batch size for training and testing\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to convert images to tensors and normalize them\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(), # Convert PIL image to tensor\n",
    "transforms.Normalize((0.4914, 0.4822, 0.4465), # Mean for each channel\n",
    "                    (0.2470, 0.2435, 0.2616)) # Std for each channel\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 training dataset with transformations applied\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "# Load the CIFAR-10 test dataset with the same transformations\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "# Create a data loader for the training set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, # Number of samples per batch\n",
    "                        shuffle=True) # Shuffle the data each epoch\n",
    "# Create a data loader for the test set\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=batch_size, # Same batch size as training\n",
    "                        shuffle=False) # No shuffling for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aaafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits the image into patches and embeds them.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, patch_size=4, emb_size=128, img_size=32):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        # We use a simple conv layer to perform patchify + embedding in one step.\n",
    "        self.proj = nn.Conv2d(in_channels, emb_size,\n",
    "        kernel_size=patch_size,\n",
    "        stride=patch_size)\n",
    "        # Number of patches\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_size))\n",
    "        # Positional embedding\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, num_patches + 1, emb_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, 3, 32, 32)\n",
    "        returns: (B, N+1, emb_size)\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        # Conv2d -> (B, emb_size, H’, W’), with H’ and W’ = 32 // patch_size\n",
    "        x = self.proj(x) # (B, emb_size, H’, W’)\n",
    "        x = x.flatten(2) # (B, emb_size, H’*W’)\n",
    "        x = x.transpose(1, 2) # (B, H’*W’, emb_size)\n",
    "        # Class token\n",
    "        cls_token = self.cls_token.expand(B, -1, -1) # (B, 1, emb_size)\n",
    "        x = torch.cat([cls_token, x], dim=1) # (B, N+1, emb_size)\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_emb[:, : x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, emb_size=128, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_size // num_heads\n",
    "        self.qkv = nn.Linear(emb_size, 3 * emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, N, emb_size)\n",
    "        B, N, _ = x.shape\n",
    "        qkv = self.qkv(x) # (B, N, 3*emb_size)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, B, num_heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # each: (B, num_heads, N, head_dim)\n",
    "        # Scaled Dot-Product Attention\n",
    "        # scores shape: (B, num_heads, N, N)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = torch.softmax(scores, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        # out shape: (B, num_heads, N, head_dim)\n",
    "        out = torch.matmul(att, v)\n",
    "        # Combine heads\n",
    "        out = out.transpose(1, 2) # (B, N, num_heads, head_dim)\n",
    "        out = out.flatten(2) # (B, N, emb_size)\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size=128, num_heads=4, expansion=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = MultiHeadSelfAttention(emb_size, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "        nn.Linear(emb_size, expansion * emb_size),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(expansion * emb_size, emb_size)\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention block\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = x_res + self.drop(x)\n",
    "        # Feed-forward block\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x_res + self.drop(x)\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels=3,\n",
    "        patch_size=4,\n",
    "        emb_size=128,\n",
    "        img_size=32,\n",
    "        num_heads=4,\n",
    "        num_layers=6,\n",
    "        num_classes=10,\n",
    "        dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_size,\n",
    "        img_size)\n",
    "        self.encoder = nn.Sequential(*[\n",
    "        TransformerEncoderBlock(\n",
    "        emb_size=emb_size,\n",
    "        num_heads=num_heads,\n",
    "        expansion=4,\n",
    "        dropout=dropout\n",
    "        ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(emb_size)\n",
    "        self.cls_head = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, 3, 32, 32)\n",
    "        x = self.patch_embed(x) # (B, N+1, emb_size)\n",
    "        x = self.encoder(x) # (B, N+1, emb_size)\n",
    "        x = self.norm(x) # (B, N+1, emb_size)aa\n",
    "        # The first token is the class token\n",
    "        cls_token_final = x[:, 0]\n",
    "        out = self.cls_head(cls_token_final) # (B, num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78233dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a ViT with default config. \n",
    "model = VisionTransformer()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train model for 20 epochs\n",
    "num_epochs = 20\n",
    "\n",
    "# plot training loss over epochs\n",
    "# plot training and testing accuracy over epochs\n",
    "# compare with CNN models and discuss observations:\n",
    "    # which model performs better?\n",
    "    # which model converges faster?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ea958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "    # train and evaluate with different hyperparameters:\n",
    "\n",
    "# a) batch size = 64, learning rate = [0.01, 0.001, 0.0001]\n",
    "    # plot training loss, training accuracya, and testing accuracy over epochs\n",
    "    # discuss observations:\n",
    "        # which learning rate performs better?\n",
    "        # which learning rate converges faster?\n",
    "# b) use the best learning rate found. Change optimizer to RMSProp.\n",
    "    # plot training loss, training accuracy, and testing accuracy over epochs\n",
    "    # discuss observations between Adam and RMSProp optimizers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. \n",
    "    # Investigate the effect of different model designs. Change num_layers to 4 and 8. \n",
    "    # plot training loss, training accuracy, and testing accuracy over epochs\n",
    "    # discuss how number of Transformer layers affects performance and convergence. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
